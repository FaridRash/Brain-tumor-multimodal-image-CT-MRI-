{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "82be84ce",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "== data.yaml ==\n",
                        "nc: 2\n",
                        "names: ['0', '1']\n",
                        "train: ../train/images\n",
                        "val:   ../valid/images\n",
                        "test:  ../test/images\n",
                        "\n",
                        "== split counts ==\n",
                        "train: images=9395, labels=9395\n",
                        "valid: images=738, labels=738\n",
                        "test: images=725, labels=725\n",
                        "\n",
                        "== annotation rows by class id ==\n",
                        "class_0: 4547\n",
                        "class_1: 6389\n",
                        "total annotation rows: 10936\n"
                    ]
                }
            ],
            "source": [
                "# Dataset sanity-check + summary (run this cell as-is)\n",
                "from pathlib import Path\n",
                "from collections import Counter\n",
                "import yaml\n",
                "\n",
                "DATASET_ROOT = Path(r\"D:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\")\n",
                "\n",
                "# 1) Read data.yaml\n",
                "data_yaml = DATASET_ROOT / \"data.yaml\"\n",
                "with open(data_yaml, \"r\", encoding=\"utf-8\") as f:\n",
                "    cfg = yaml.safe_load(f)\n",
                "\n",
                "print(\"== data.yaml ==\")\n",
                "print(f\"nc: {cfg.get('nc')}\")\n",
                "print(f\"names: {cfg.get('names')}\")\n",
                "print(f\"train: {cfg.get('train')}\")\n",
                "print(f\"val:   {cfg.get('val')}\")\n",
                "print(f\"test:  {cfg.get('test')}\")\n",
                "\n",
                "# 2) Count files per split\n",
                "print(\"\\n== split counts ==\")\n",
                "for split in [\"train\", \"valid\", \"test\"]:\n",
                "    img_dir = DATASET_ROOT / split / \"images\"\n",
                "    lbl_dir = DATASET_ROOT / split / \"labels\"\n",
                "    img_count = len(list(img_dir.glob(\"*\")))\n",
                "    lbl_count = len(list(lbl_dir.glob(\"*.txt\")))\n",
                "    print(f\"{split}: images={img_count}, labels={lbl_count}\")\n",
                "\n",
                "# 3) Count annotation rows per class id\n",
                "class_counts = Counter()\n",
                "label_files = list((DATASET_ROOT / \"train\" / \"labels\").glob(\"*.txt\")) \\\n",
                "            + list((DATASET_ROOT / \"valid\" / \"labels\").glob(\"*.txt\")) \\\n",
                "            + list((DATASET_ROOT / \"test\" / \"labels\").glob(\"*.txt\"))\n",
                "\n",
                "for lf in label_files:\n",
                "    with open(lf, \"r\", encoding=\"utf-8\") as f:\n",
                "        for line in f:\n",
                "            line = line.strip()\n",
                "            if not line:\n",
                "                continue\n",
                "            cls_id = line.split()[0]\n",
                "            class_counts[cls_id] += 1\n",
                "\n",
                "print(\"\\n== annotation rows by class id ==\")\n",
                "for k in sorted(class_counts.keys(), key=int):\n",
                "    print(f\"class_{k}: {class_counts[k]}\")\n",
                "print(f\"total annotation rows: {sum(class_counts.values())}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "b528f22a",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "3252f199",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Ultralytics 8.4.14  Python-3.13.11 torch-2.10.0+cpu CPU (Intel Core i7-8850H 2.60GHz)\n",
                        "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=D:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=yolov8n_seg_baseline4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs_brain_ct, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=D:\\My Projects\\Brain-tumor-multimodal-image-CT-MRI-\\Notebook\\runs\\segment\\runs_brain_ct\\yolov8n_seg_baseline4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
                        "Overriding model.yaml nc=80 with nc=2\n",
                        "\n",
                        "                   from  n    params  module                                       arguments                     \n",
                        "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
                        "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
                        "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
                        "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
                        "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
                        "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
                        "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
                        "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
                        "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
                        "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
                        " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
                        " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
                        " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
                        " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
                        " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
                        " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
                        " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
                        " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
                        " 22        [15, 18, 21]  1   1004470  ultralytics.nn.modules.head.Segment          [2, 32, 64, 16, None, [64, 128, 256]]\n",
                        "YOLOv8n-seg summary: 152 layers, 3,264,006 parameters, 3,263,990 gradients, 11.5 GFLOPs\n",
                        "\n",
                        "Transferred 381/417 items from pretrained weights\n",
                        "Freezing layer 'model.22.dfl.conv.weight'\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 337.3137.4 MB/s, size: 58.3 KB)\n",
                        "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\train\\labels.cache... 9395 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 9395/9395 2.1Git/s 0.0s\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\train\\images\\CANAQUIRI-ANDOA0001_jpg.rf.27e1046e37e5bd880dd7cc8aed7f3741.jpg: 1 duplicate labels removed\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\train\\images\\CANAQUIRI-ANDOA0001_jpg.rf.391d0088a176c59ce045d0ac51ff8754.jpg: 1 duplicate labels removed\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\train\\images\\CANAQUIRI-ANDOA0001_jpg.rf.5bf8b3f5918f8b6f2e9753dcbebada6e.jpg: 1 duplicate labels removed\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\train\\images\\CANAQUIRI-ANDOA0001_jpg.rf.7b0150af1d10844432c3b0e323792518.jpg: 1 duplicate labels removed\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\train\\images\\CANAQUIRI-ANDOA0001_jpg.rf.7d7783630cd54b22848b479ffab7e7b3.jpg: 1 duplicate labels removed\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\train\\images\\CANAQUIRI-ANDOA0001_jpg.rf.89ac7b546cdaa56fe29d0dcfd744c3f3.jpg: 1 duplicate labels removed\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\train\\images\\CANAQUIRI-ANDOA0001_jpg.rf.a575abc671e908f5a4f0e6d3f8387b2d.jpg: 1 duplicate labels removed\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\train\\images\\CANAQUIRI-ANDOA0001_jpg.rf.a7e3ee5ddeb3f7d65a088742f34d6ac3.jpg: 1 duplicate labels removed\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\train\\images\\CANAQUIRI-ANDOA0001_jpg.rf.fae095762b1494565c47494047cc117d.jpg: 1 duplicate labels removed\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\train\\images\\CIEZ-ANNAY0004_jpg.rf.3c06f01f48f0b4f9ebccc20b61363f86.jpg: 1 duplicate labels removed\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\train\\images\\CIEZ-ANNAY0004_jpg.rf.8510809d452b8c8a0263994a8c1190fe.jpg: 1 duplicate labels removed\n",
                        "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 440.7130.3 MB/s, size: 52.7 KB)\n",
                        "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\valid\\labels.cache... 738 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 738/738 162.9Mit/s 0.0s\n",
                        "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
                        "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
                        "Plotting labels to D:\\My Projects\\Brain-tumor-multimodal-image-CT-MRI-\\Notebook\\runs\\segment\\runs_brain_ct\\yolov8n_seg_baseline4\\labels.jpg... \n",
                        "Image sizes 640 train, 640 val\n",
                        "Using 0 dataloader workers\n",
                        "Logging results to \u001b[1mD:\\My Projects\\Brain-tumor-multimodal-image-CT-MRI-\\Notebook\\runs\\segment\\runs_brain_ct\\yolov8n_seg_baseline4\u001b[0m\n",
                        "Starting training for 50 epochs...\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
                        "\u001b[K       1/50         0G      1.061      3.821      3.179      1.581          0         15        640: 1% ──────────── 15/1175 4.8s/it 1:17<1:32:506\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m data_yaml = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mMy Projects\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mBrain Dataset\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mCT-Brain-Segmentation-1\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdata.yaml\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m model = YOLO(\u001b[33m\"\u001b[39m\u001b[33myolov8n-seg.pt\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# small/fast starter model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_yaml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mruns_brain_ct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43myolov8n_seg_baseline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\farid\\miniconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:774\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    771\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    772\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\farid\\miniconda3\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:244\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    241\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\farid\\miniconda3\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:445\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    440\u001b[39m         \u001b[38;5;28mself\u001b[39m.tloss = (\n\u001b[32m    441\u001b[39m             \u001b[38;5;28mself\u001b[39m.loss_items \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.tloss * i + \u001b[38;5;28mself\u001b[39m.loss_items) / (i + \u001b[32m1\u001b[39m)\n\u001b[32m    442\u001b[39m         )\n\u001b[32m    444\u001b[39m     \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m torch.cuda.OutOfMemoryError:\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m epoch > \u001b[38;5;28mself\u001b[39m.start_epoch \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._oom_retries >= \u001b[32m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m RANK != -\u001b[32m1\u001b[39m:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\farid\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\farid\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\farid\\miniconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "# 2) Train YOLOv8 segmentation\n",
                "from ultralytics import YOLO\n",
                "\n",
                "data_yaml = r\"D:\\My Projects\\Brain Dataset\\CT-Brain-Segmentation-1\\data.yaml\"\n",
                "model = YOLO(\"yolov8n-seg.pt\")  # small/fast starter model\n",
                "\n",
                "model.train(\n",
                "    data=data_yaml,\n",
                "    epochs=50,\n",
                "    imgsz=640,\n",
                "    batch=8,\n",
                "    project=\"runs_brain_ct\",\n",
                "    name=\"yolov8n_seg_baseline\"\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0468419",
            "metadata": {},
            "outputs": [],
            "source": [
                "model.val(data=data_yaml)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0c92478c",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
